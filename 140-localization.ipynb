{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmore import layers, flex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# LOCALIZATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# OCR Training Data\n",
    "\n",
    "OCR training data usually consists of:\n",
    "\n",
    "- an image of text\n",
    "- a transcription of the text\n",
    "\n",
    "We are usually not given:\n",
    "\n",
    "- character bounding boxes\n",
    "- word bounding boxes (when recognizing text lines)\n",
    "- page segmentation (when recognizing whole pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# EM Algorithms\n",
    "\n",
    "Expectation-Maximization is a general approach to learning when some variable we need for recognition is missing from the training data.\n",
    "\n",
    "For OCR, the missing information is the _segmentation_ (e.g., character locations.\n",
    "\n",
    "EM approach:\n",
    "- make a first guess at the segmentation\n",
    "- recognize assuming the segmentation is correct\n",
    "- update the segmentation using the recognition output\n",
    "- repeat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# CTC as an EM Algorithm\n",
    "\n",
    "- perform scanning recognition (e.g. with LSTM)\n",
    "- perform Viterbi/CTC alignment to find the best locations for each character\n",
    "- use those locations as if they were ground truth\n",
    "- compute cross entroy / MSE loss and backpropagate\n",
    "\n",
    "CTC gives us horizontal positions of characters but no vertical locations.\n",
    "\n",
    "What if we want XY positions for each character? Bounding boxes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Brute Force XY Character Positions\n",
    "\n",
    "- input: unnormalized word images\n",
    "- get X positions from CTC algorithm / DL output\n",
    "- formally assign Y positions to characters as $\\mu_y$ of the word image\n",
    "- train a convolutional neural network using $(x_{CTC}, \\mu_y)$ as the location for each character\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# EM Algorithm for XY Character Positions\n",
    "\n",
    "- input: unnormalized word images\n",
    "- have a 2D convolutional network that outputs the probability of the presence of a character at each pixels $(x, y)$\n",
    "- reduce the 2D probability map to a 1D probability sequence\n",
    "- perform CTC on the 1D sequence as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_probabilities_2d_to_1d(outputs):\n",
    "    # outputs in BDHW format\n",
    "    l = outputs.softmax(1)\n",
    "    r1 = l[:,1:,:,:].max(2)[0] # BDW\n",
    "    r0 = (1-r1.max(1)[0])[:,None,:] # B1W\n",
    "    z = torch.cat([r0, r1], dim=1).log().softmax(1)\n",
    "    # z in BDL format\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# RCNN-like Algorithm\n",
    "\n",
    "- you can implement region proposal algorithms directly for character bounding boxes\n",
    "- the problem is complicated by the fact that there are frequently multiple instances of each character\n",
    "- a direct implementation does not take advantage of the known left-to-right ordering of the transcript but simply treats transcripts as bags of characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Segmentation by Backpropagation / Masking\n",
    "\n",
    "Several techniques in the literature have been developed to determine which regions in the input are responsible for a given output class:\n",
    "\n",
    "- for each output location of a CTC-trained model, compute the derivative of the output with respect to the input pixels\n",
    "- scan a mask across the input image and determine which mask locations affect which character classification\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
