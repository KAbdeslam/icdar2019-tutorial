{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmore import flex, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# APPLICATIONS TO OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Character Recognition\n",
    "\n",
    "- assuming you have a character segmentation\n",
    "  - extract each character\n",
    "  - feed to any of these architectures as if it were an object recognition problem\n",
    "\n",
    "Goodfellow, Ian J., et al. \"Multi-digit number recognition from street view imagery using deep convolutional neural networks.\" arXiv preprint arXiv:1312.6082 (2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Whole Word Recognition\n",
    "\n",
    "- perform word localization using Faster RCNN\n",
    "- perform whole word recognition as if it were a large object reconition problem\n",
    "\n",
    "![word recognition](figs/word-recognition.png)\n",
    "\n",
    "Jaderberg, Max, et al. \"Deep structured output learning for unconstrained text recognition.\" arXiv preprint arXiv:1412.5903 (2014)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Better Techniques\n",
    "\n",
    "- above techniques are applications of computer vision localization\n",
    "- Faster RCNN and similar techniques are ad hoc and limited\n",
    "- often require pre-segmented text for training\n",
    "- better approaches:\n",
    "  - use markers for localizing/bounding text (later)\n",
    "  - use sequence learning techniques and CTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Networks for OCR\n",
    "\n",
    "Historically, LSTM came first, but we're going to start off with convolutional networks analogous to object recognition networks.\n",
    "\n",
    "Structure:\n",
    "\n",
    "- perform 2D convolutions over the entire image\n",
    "- assume that that has extracted features that correspond to characters\n",
    "- project those features into a 1D sequence\n",
    "- classify the projected 1D feature sequence into characters\n",
    "- perform training using EM alignment (CTC, Viterbi, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Networks for OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    return nn.Sequential(\n",
    "        # BDHW\n",
    "        *convolutional_layers(),\n",
    "        # BDHW, now reduce along the vertical\n",
    "        layers.Fun(lambda x: x.sum(2)),\n",
    "        # BDW\n",
    "        layers.Conv1d(num_classes, 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Viterbi Training\n",
    "\n",
    "- ground truth: text string = sequence of classes\n",
    "- ground truth `\"ABC\"` is replaced by regular expression `/_+A+_+B+_+C+_+/`\n",
    "- network outputs $P(c|i)$, a probability of each class $c$ at each position $i$\n",
    "- find the best possible alignment between network outputs and ground truth regex\n",
    "- that alignment gives an output for each time step\n",
    "- treat that alignment as if it were the ground truth and backpropagate\n",
    "- this is an example of an EM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CTC Training\n",
    "\n",
    "- like Viterbi training, but instead of finding the best alignment uses an average alignment\n",
    "\n",
    "Identical to traditional HMM training in speech recognition:\n",
    "\n",
    "- Viterbi training = Viterbi training\n",
    "- CTC training = forward-backward algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CTC training with cctc2\n",
    "\n",
    "- with the `cctc2` library, we can make the alignment explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def train_batch(input, target):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input)\n",
    "    aligned = cctc2.align(output, target)\n",
    "    loss = mse_loss(aligned, output)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CTC in PyTorch\n",
    "\n",
    "- in PyTorch, CTC is implemented as a loss function\n",
    "- `CTCLoss` in PyTorch obscures what's going on\n",
    "- all you get is the loss output, not the EM alignment\n",
    "- sequences are packed in a special way into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ctc_loss = CTCLoss()\n",
    "\n",
    "def train_batch(input, target):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input)\n",
    "    loss = ctc_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word / Text Line Recognition  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    return nn.Sequential(\n",
    "        *convolutional_layers(),\n",
    "        layers.Fun(lambda x: x.sum(2)),\n",
    "        layers.Conv1d(num_classes, 1)\n",
    "    )\n",
    "\n",
    "def train_batch(input, target):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input)\n",
    "    loss = ctc_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# VGG-Like Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def make_vgg_model():\n",
    "    return nn.Sequential(\n",
    "        layers.Input(\"BDHW\", sizes=[None, 1, None, None]),\n",
    "        *conv2mp(100, 3, 2, repeat=2),\n",
    "        *conv2mp(200, 3, 2, repeat=2),\n",
    "        *conv2mp(300, 3, 2, repeat=2),\n",
    "        *conv2d(400, 3, repeat=2),\n",
    "        *project_and_conv1d(800, noutput)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resnet-Block\n",
    "\n",
    "- NB: we can easily define Resnet etc. in an object-oriented fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def ResnetBlock(d, r=3):\n",
    "    return nn.Sequential(\n",
    "        Additive(\n",
    "            nn.Identity(),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(d, d, r, padding=r//2), nn.BatchNorm2d(d), nn.ReLU(),\n",
    "                nn.Conv2d(d, d, r, padding=r//2), nn.BatchNorm2d(d)\n",
    "            )\n",
    "        ),\n",
    "        nn.MaxPool2d(2)\n",
    "    )\n",
    "\n",
    "def resnet_blocks(n, d, r=3):\n",
    "    return [ResnetBlock(d, r) for _ in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resnet-like Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def make_resnet_model():    \n",
    "    return nn.Sequential(\n",
    "        layers.Input(\"BDHW\", sizes=[None, 1, None, None]),\n",
    "        *conv2mp(64, 3, (2, 1)),\n",
    "        *resnet_blocks(5, 64), *conv2mp(128, 3, (2, 1)),\n",
    "        *resnet_blocks(5, 128), *conv2mp(256, 3, 2),\n",
    "        *resnet_blocks(5, 256), *conv2d(256, 3),\n",
    "        *project_and_conv1d(800, noutput)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Footprints\n",
    "\n",
    "- even with projection/1D convolution, a character is first recognized in 2D by the 2D convolutional network\n",
    "- character recognition with 2D convolutional networks really a kind of deformable template matching\n",
    "- in order to recognize a character, each pixel at the output of the 2D convolutional network needs to have a footprint large enough to cover the character to be recognized\n",
    "- footprint calculation:\n",
    "  - 3x3 convolution, three maxpool operations = 24x24 footprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv-Only Training\n",
    "\n",
    "<img src=\"figs/conv-only.png\" style=\"height: 6in\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problems with VGG/Resnet+Conv1d\n",
    "\n",
    "Problem:\n",
    "- reduces output to H/8, W/8\n",
    "- CTC alignment needs two pixels for each character\n",
    "- these models has trouble with narrow characters\n",
    "\n",
    "Solutions:\n",
    "- use fractional max pooling\n",
    "- use upscaling\n",
    "- use transposed convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Less Downscaling using `FractionalMaxPool2d`\n",
    "\n",
    "- permits more max pooling steps without making image too small\n",
    "- can be performed anisotropically\n",
    "- necessary non-uniform spacing may have additional benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def conv2fmp(d, r, ratio=(0.7, 0.85)):\n",
    "    return [\n",
    "        flex.Conv2d(d, r, padding=r//2), flex.BatchNorm2d(), nn.ReLU(),\n",
    "        nn.FractionalMaxPool2d(3, ratio)\n",
    "    ]\n",
    "\n",
    "def make_fmp_model():\n",
    "    return nn.Sequential(\n",
    "        layers.Input(\"BDHW\", sizes=[None, 1, None, None]),\n",
    "        *conv2fmp(50, 3), *conv2fmp(100, 3), *conv2fmp(150, 3), *conv2fmp(200, 3),\n",
    "        layers.Fun(\"lambda x: x.sum(2)\"), # BDHW -> BDW\n",
    "        *project_and_conv1d(800, noutput)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Upscaling using `interpolate`\n",
    "\n",
    "- `interpolate` scales an image, has `backward()`\n",
    "- `MaxPool2d...interpolate` is a simple multiscale analysis\n",
    "- can be combined with loss functions at each level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_interpolating_model():\n",
    "    return nn.Sequential(\n",
    "        layers.Input(\"BDHW\", sizes=[None, 1, None, None]),\n",
    "        *conv2mp(50, 3), *conv2mp(100, 3), *conv2mp(150, 3), *conv2mp(200, 3),\n",
    "        layers.Fun(\"lambda x: x.interpolate(x, scale=4)\")\n",
    "        *project_and_conv1d(800, noutput)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Upscaling with `interpolate`\n",
    "\n",
    "<img src=\"figs/conv-keep.png\" style=\"height: 6in\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Upscaling using `ConvTranspose1d`\n",
    "\n",
    "\n",
    "- `ConvTranspose2d` fills in higher resolutions with \"templates\"\n",
    "- commonly used in image segmentation and superresolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def make_ct_model():\n",
    "    return nn.Sequential(\n",
    "        layers.Input(\"BDHW\", sizes=[None, 1, None, None]),\n",
    "        *conv2mp(50, 3), \n",
    "        *conv2mp(100, 3),\n",
    "        *conv2mp(150, 3),\n",
    "        *conv2mp(200, 3),\n",
    "        layers.Fun(\"lambda x: x.sum(2)\"), # BDHW -> BDW\n",
    "        flex.ConvTranspose1d(800, 1, stride=2), # <-- undo too tight spacing\n",
    "        *project_and_conv1d(800, noutput)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How well do these work?\n",
    "\n",
    "- Works for word or text line recognition.\n",
    "- All these models only require that characters are arranged left to right.\n",
    "- Input images can be rotated up to around 30 degrees and scaled.\n",
    "- Input images can be grayscale.\n",
    "- Great for scene text and degraded documents.\n",
    "\n",
    "But:\n",
    "- You pay a price for translation/scale inv: lower performance.\n",
    "- These don't use any recurrent networks, so all information flow is strictly limited to the footprint.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
